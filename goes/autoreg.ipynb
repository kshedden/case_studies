{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# GOES autoregression modeling\n",
    "\n",
    "[Autoregressive models](https://en.wikipedia.org/wiki/Autoregressive_model) are one of the most common forms of time series model.  The most familiar autoregressive (AR) model is a linear autoregressive model, but there are many forms of non-linear autoregressive model and this area remains highly active in terms of research.\n",
    "\n",
    "Here we use linear autoregressive models to assess the information in past flux measurements for predicting the current flux measurement.  We incorporate a gap between the past measurements, 'x' and the present measurement 'y'. This analysis provides insight into how the information in the past about the present is best summarized and represented.\n",
    "\n",
    "Since linear AR models naturally exhibit multicollinearity (due to autocorrelation), we use [ridge regression](https://en.wikipedia.org/wiki/Ridge_regression) here to fit the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from scipy.stats.distributions import chi2\n",
    "from read import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "We will use ridge regression to fit our autoregression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def ridge(x, y, f):\n",
    "    \"\"\"Regress y on x using ridge regression, with penalty parameter f.\"\"\"\n",
    "    u, s, vt = np.linalg.svd(x, 0)\n",
    "    v = vt.T\n",
    "    g = s / (s**2 + f)\n",
    "    b = np.dot(v, np.dot(u.T, y) * g)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Parameter tuning is an important practical issue for most modern forms of statistical modeling.  The question of how to tune ridge regression has been extensively considered, but remains challenging.  A fairly [recent paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4790465/) proposes a new way to train ridge regression and evaluates the tuning method quite extensively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Ridge regression with tuning based on a hyperpenaized log likelihood.\n",
    "# Requires a guess of the R^2 (r2).\n",
    "def tuneridge(x, y, r2):\n",
    "\n",
    "    n, p = x.shape\n",
    "\n",
    "    # The hyperpenalty parameters\n",
    "    a = p / 2\n",
    "    b = 1 / np.sqrt(1/r2 - 1)\n",
    "\n",
    "    # Estimate the parameters for fixed penalty value using ridge regression.\n",
    "    u, s, vt = np.linalg.svd(x, 0)\n",
    "    v = vt.T\n",
    "    def fit(lam):\n",
    "        g = s / (s**2 + lam)\n",
    "        return np.dot(v, np.dot(u.T, y) * g)\n",
    "\n",
    "    lam = 1.\n",
    "    bhat = fit(lam)\n",
    "    for itr in range(100):\n",
    "        resid = y - np.dot(x, bhat)\n",
    "        rss = np.sum(resid**2)\n",
    "        bss = np.sum(bhat**2)\n",
    "        \n",
    "        # Equation 21\n",
    "        sigma2 = (rss + lam*bss) / (n + p + 2)\n",
    "        \n",
    "        # Equation 24\n",
    "        lam = (p + 2*a - 2) / (bss/sigma2 + 2*b)\n",
    "        \n",
    "        bhat1 = fit(lam)\n",
    "        if np.sum((bhat1 - bhat)**2) < 1e-4:\n",
    "            break\n",
    "        bhat = bhat1\n",
    "\n",
    "    return bhat, lam, sigma2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Below we use a very limited simulation study to assess the performance of the tuning procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def test_tuneridge1(n, p, r, sig):\n",
    "    x = np.random.normal(size=(n, p))\n",
    "    for j in range(1, p):\n",
    "        x[:, j] = r*x[:, j-1] + np.sqrt(1-r**2)*x[:, j]\n",
    "    b = np.zeros(p)\n",
    "    b[0] = 1\n",
    "    b[1] = -1\n",
    "    Ey = np.dot(x, b)\n",
    "    y = Ey + sig*np.random.normal(size=n)\n",
    "\n",
    "    r2 = np.var(Ey) / (np.var(Ey) + sig**2)\n",
    "    ii = np.arange(p)\n",
    "    covx = r**np.abs(np.subtract.outer(ii, ii))\n",
    "\n",
    "    # The large sample optimal lambda (section 4.1)\n",
    "    lam_opt = sig**2 * np.trace(np.linalg.inv(covx)) / np.dot(b, np.linalg.solve(covx, b))\n",
    "\n",
    "    bhat, lam, sigma2 = tuneridge(x, y, r2)\n",
    "    return lam, lam_opt, r2, bhat\n",
    "\n",
    "def test_tuneridge(n, p, r, sig, nrep=100):\n",
    "    lam, lam_opt, r2, bhat = [], [], [], []\n",
    "    for k in range(nrep):\n",
    "        lam1, lam_opt1, r21, bhat1 = test_tuneridge1(n, p, r, sig)\n",
    "        lam.append(lam1)\n",
    "        lam_opt.append(lam_opt1)\n",
    "        r2.append(r21)\n",
    "        bhat.append(bhat1)\n",
    "    return np.asarray(lam), np.asarray(lam_opt), np.asarray(r2), np.asarray(bhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "p = 5\n",
    "r = 0.5\n",
    "sig = 2\n",
    "lam, lam_opt, r2, bhat = test_tuneridge(n, p, r, sig)\n",
    "print(\"Expected parameter estimate:\\n\", bhat.mean(0))\n",
    "print(\"Standard error of parameter estimate:\\n\", bhat.std(0))\n",
    "print(\"lambda:\\n\", lam.mean(), \" \", lam.std())\n",
    "print(\"optimal lambda:\\n\", lam_opt.mean(), \" \", lam_opt.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Now we proced to use linear autoregression, to assess the conditional dependence structure of the flux series in the GOES study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_goes(2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Use blocks of size m, and use the first q observations within each block to predict the final observation in the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1000\n",
    "q = 200\n",
    "\n",
    "print(\"Gap between final x and y: \", )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The time points of the predictor information relative to the time being predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tax = np.arange(-2*m, -2*(m-q))[0:q] / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Make blocks of 'm' consecutive time points with approximately 2-second spacing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tix, flx = make_blocks(df, m, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "flx = np.log(1e-8 + flx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Create a design matrix and response vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = flx[:, 0:q]\n",
    "y = flx[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Center the data.  This is very common in ridge regression and allows us to do the analysis without including an intercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "y -= y.mean()\n",
    "x -= x.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "xtx = np.dot(x.T, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Consider how the regression coefficients look for various values\n",
    "# of the penalty parameter, as specified through the value of r^2.\n",
    "def fit(x, y, randomize):\n",
    "\n",
    "    if randomize:\n",
    "        ii = np.random.permutation(len(y))\n",
    "        y = y[ii]\n",
    "\n",
    "    for r2 in [0.01, 0.1, 0.25, 0.5, 0.75, 0.9]:\n",
    "        b, lam, s2 = tuneridge(x, y, r2)\n",
    "\n",
    "        H = xtx + lam*np.eye(q)\n",
    "        V = s2 * np.linalg.solve(H, np.linalg.solve(H, xtx).T)\n",
    "        cs = np.dot(b, np.linalg.solve(V, b))\n",
    "        pv = 1 - chi2(len(b)).cdf(cs)\n",
    "\n",
    "        plt.clf()\n",
    "        plt.grid(True)\n",
    "        plt.plot(tax, b, \"-\")\n",
    "        plt.ylabel(\"Coefficient\", size=15)\n",
    "        plt.xlabel(\"Minutes before current time\", size=15)\n",
    "        plt.title(\"r2=%.2f, lambda=%.1f\" % (r2, lam))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "First we fit the models to the actual GOES flux data, using a series of $r^2$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(x, y, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Now we fit the models to data in which the response values $y$ have been randomized.  These allow us to assess the level of estimation noise in the parameter estimates.  Potentially the values below could be used to identify a level beyond which parameter estimates would be deemed to be statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(x, y, True)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
